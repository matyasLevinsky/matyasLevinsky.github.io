[
  {
    "objectID": "SlerkaProjekt.html",
    "href": "SlerkaProjekt.html",
    "title": "Analyzing the CNN coverage of the Prague University shooting",
    "section": "",
    "text": "Sensitive content\n\n\n\nThis Article, while only analyzing the reporting about a mass shooting will still contain some references about it. Read with caution if you are sensitive to that.\nGiven that the University shooting that happened on the 21 December 2023 was unprecedented in Czech history, I wondered whether it would be reflected as such in american media that has grown increasingly desensitized to news of mass shootings."
  },
  {
    "objectID": "SlerkaProjekt.html#getting-the-data",
    "href": "SlerkaProjekt.html#getting-the-data",
    "title": "Analyzing the CNN coverage of the Prague University shooting",
    "section": "Getting the data",
    "text": "Getting the data\nI wanted to compare the mass shooting to comparable reporting about mass shootings using the same live-block format CNN used for the Prague shooting. I used the first 12 articles that popped up for the Google search ‘site:cnn.com “mass shooting” live-news’ and filtering for articles in 2023-2024.\nI created a list of the necessary URLs and a couple of helper functions: readUrlList, parseUrlList and generateTeiCorpus. They are simple for-loops that do what their name suggest. Let’s load them together with the packages we will need.\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(janitor)\nlibrary(here)\nsource(here(\"_data/helperCNNAnalysis.R\"))\n\nNow we can use them to read and parse the HTML of the CNN live-blogs. This may take some time so lets save it for later. If the code should be broken I have provided my own webscrape from 28.02.2024.\n\nhtmlList &lt;- readUrlList(urlList, silent = T)\nparsedDt &lt;- parseUrlList(htmlList, silent = T)\nsaveRDS(parsedDt, file = \"_data/parsedDt_WebscrapingCNN.rds\")\nparsedDt\n\nAlternative:\n\nparsedDt &lt;- readRDS(here(\"_data/parsedDt_WebscrapingCNN.rds\"))\nparsedDt\n\n\n  \n\n\n\nAs we can see, we don’t really see the article text (the last column “liveBlogUpdate”) we need to un-nest the data and fix the column names while we are at it.\n\nblogPostsExpanded &lt;- parsedDt %&gt;%\n  unnest(cols = c(liveBlogUpdate), names_sep = \"_blog\") %&gt;% \n  rename_with(~ str_replace_all(.x, \"[@\\\\$]\", \"\")) %&gt;% \n  select(mainEntityOfPage:datePublished, author:liveBlogUpdate_blogheadline, liveBlogUpdate_blogauthor:liveBlogUpdate_blogarticleBody)\n\nblogPostsExpanded"
  },
  {
    "objectID": "SlerkaProjekt.html#tei-conversion",
    "href": "SlerkaProjekt.html#tei-conversion",
    "title": "Analyzing the CNN coverage of the Prague University shooting",
    "section": "TEI conversion",
    "text": "TEI conversion\nSome tools require a TEI compatible corpus such as: https://voyant-tools.org/. We will now convert our data:\n\nblogTEI &lt;- blogPostsExpanded %&gt;% \n  mutate(\n    MetadataColumn = paste0(\"eventName: \", about$name, \"headline:\", liveBlogUpdate_blogheadline), \n    TextColumn = liveBlogUpdate_blogarticleBody\n  )\n\ndescription &lt;- paste0(\"A TEI corpus containg \", length(blogTEI), \" live blogpost from CNN about mass shootings.\")\n\nprague &lt;- generateTeiCorpus(MetadataColumn = blogTEI$MetadataColumn, TextColumn = blogTEI$TextColumn, MetadataSting = description, silent = T)"
  },
  {
    "objectID": "SlerkaProjekt.html#analysis-in-r",
    "href": "SlerkaProjekt.html#analysis-in-r",
    "title": "Analyzing the CNN coverage of the Prague University shooting",
    "section": "Analysis in R",
    "text": "Analysis in R\nIf we want to keep analyzing in R we have multiple options. I would refer you to the great book ‘Text Mining with R’. Let’s use the tidytext and textdata packages to do a simple sentiment analysis. First we need to shape our data accordingly.\n\nlibrary(tidytext)\nlibrary(textdata)\n\n# Analysis\nreducedTb &lt;- blogPostsExpanded %&gt;% \n  unnest(cols = c(about)) %&gt;% \n  select(name, liveBlogUpdate_blogheadline, liveBlogUpdate_blogarticleBody) %&gt;% \n  rename(\"headline\" = liveBlogUpdate_blogheadline, \"text\" = liveBlogUpdate_blogarticleBody) %&gt;% \n  mutate(\n    text = tolower(text),\n    name = gsub(\"April 10, 2023: |April 11, 2023 - |Live updates: |March 27, 2023 |May 3, 2023 - |October 25, 2023 - \", \"\", name), \n    name = str_wrap(name, width = 40)\n    ) %&gt;%\n  unnest_tokens(word, text) %&gt;% \n  anti_join(stop_words, by = \"word\")\n\nNrcSentiments &lt;- get_sentiments(\"nrc\") # Get the sentiment database\n\n# Join our webscraped date with the sentiment file\narticlesSentiment &lt;- reducedTb %&gt;%\n  inner_join(NrcSentiments, by = \"word\", relationship = \"many-to-many\") %&gt;%\n  count(name, sentiment) %&gt;%\n  pivot_wider(names_from = sentiment, values_from = n) %&gt;% \n  mutate(\n    total = rowSums(select(., anger:trust)),\n    other = rowSums(select(., c(disgust, joy, surprise))),\n    reducedTb %&gt;% group_by(name) %&gt;% summarise(articles = n_distinct(headline)) %&gt;% select(\"articles\"), # Direct add articles\n    across(anger:other, ~ .x / total, .names = \"{.col}Prop\")\n  )\n\n# Pivot into a long format for the purpose of plotting\narticlesSentimentLong &lt;- articlesSentiment %&gt;% \n  select(name, angerProp, anticipationProp, fearProp, negativeProp:sadnessProp, trustProp, otherProp) %&gt;% \n  pivot_longer(cols = angerProp:otherProp,  names_to = \"emotion\", values_to = \"prevalence\") %&gt;% \n  mutate(emotion = gsub(\"Prop\", \"\", emotion))\n\nunique(articlesSentimentLong$emotion)\n\n[1] \"anger\"        \"anticipation\" \"fear\"         \"negative\"     \"positive\"    \n[6] \"sadness\"      \"trust\"        \"other\"       \n\n\nWe have reduced our data and joined it with the sentiment database nrc published by Saif Mohammad and Peter Turney. After that we computed rowSums and ‘rowAverages’, finally we converted the data into a long format for the purpose of plotting. For that we will be using the packages ggplot2 and scales.\n\nlibrary(ggplot2)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nggplot(articlesSentimentLong) +\n  aes(x = name, y = prevalence, fill = emotion) +\n  geom_col() +\n  geom_text(\n    aes(label = percent(prevalence, accuracy = 0.1)),\n    nudge_y = if_else(articlesSentimentLong$prevalence &gt; .15, -.05, .05),  \n    size = 3, \n    color =  \"black\") + \n  coord_flip() +\n  theme_minimal() + \n  theme(axis.ticks = element_blank(), \n        axis.text.x = element_blank(), \n        axis.title.x = element_blank(), \n        axis.title.y = element_blank(), \n        legend.position = \"none\") +\n  facet_grid(~ emotion)"
  },
  {
    "objectID": "SlerkaProjekt.html#expanding-on-that-analysis-latent-dirichlet-allocation",
    "href": "SlerkaProjekt.html#expanding-on-that-analysis-latent-dirichlet-allocation",
    "title": "Analyzing the CNN coverage of the Prague University shooting",
    "section": "Expanding on that analysis: Latent Dirichlet allocation",
    "text": "Expanding on that analysis: Latent Dirichlet allocation\nLatent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. We will use it to give some oomph to our analysis. For that we will be using the package topicmodels. We will also have to adapt the shape of our data:\n\nlibrary(topicmodels)\n\nremoveWords &lt;- c(\"cnn\", \"p.m\")\ntoRemoveDf &lt;- tibble(document = NA, word = removeWords, n = 1)\n\n# Dataformatting\n# Need one-term-per-document-per-row\n\nDtmData &lt;- reducedTb %&gt;% \n  group_by(name, headline) %&gt;% \n  count(word, sort = T) %&gt;% \n  ungroup() %&gt;% \n  mutate(document = paste0(name, \"_\", headline)) %&gt;% \n  select(document, word, n) %&gt;% \n  anti_join(toRemoveDf, by = \"word\")\n\nhead(DtmData, 5)\n\n\n  \n\n\n\nNow that our data is in an appropriate format we can start modeling. As we know that there are 12 Events we have scraped from the Web we can input that for gaining an optimal model. (I have attempted other values, the results were less than stellar.)\n\ncastData &lt;- cast_tdm(data = DtmData, term = document, document = word, value = n) # reverse necessary\n\ncastData\n\n&lt;&lt;TermDocumentMatrix (terms: 432, documents: 3991)&gt;&gt;\nNon-/sparse entries: 20266/1703846\nSparsity           : 99%\nMaximal term length: 180\nWeighting          : term frequency (tf)\n\nmodel1 &lt;- LDA(castData, k = 12, control = list(seed = 1234)) # reverses, see above\n  \nmodel1\n\nA LDA_VEM topic model with 12 topics.\n\nmodel1Topics &lt;- tidytext::tidy(model1, matrix = \"beta\")\n\nmodel1Topics\n\n\n  \n\n\n\nYou might have noticed the issue with the reverse naming of document and word. For some reason there happens a switch in the LDA() step, that I was unable to diagnose, so this is my shoddy workaround. Now that we have the model we can look how certain terms are present in the models 12 topics. Remember that the topics were set by the model itself and not by us. Therefor we can evaluate if the topic are sensible to our human preferences.\n\nshootingTopTerms &lt;- model1Topics %&gt;%\n  group_by(topic) %&gt;%\n  slice_max(beta, n = 10) %&gt;% \n  ungroup() %&gt;%\n  arrange(topic, -beta)\n\nshootingTopTerms %&gt;%\n  mutate(term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\n\n\nWe are mostly interested in plot nr. 11 that is showing the language used during the reporting on the Prague mass shooting. To my sensibilities the graph make sense and so I will continue with the analysis. To determine whether the Prague shooting was a routing reporting experience for the writers at CNN we can look whether the articles themselves are significantly different. For that purpose we will separate the individual articles from their Events and look how many the model can correctly allocate to the correct topic.\n\nmodel1Gamma &lt;- tidytext::tidy(model1, matrix = \"gamma\")\n\nmodel1Gamma\n\n\n  \n\n\nseparatedGamma &lt;- model1Gamma %&gt;%\n  separate(document, c(\"Event\", \"Headline\"), sep = \"_\", convert = TRUE)\n\nseparatedGamma\n\n\n  \n\n\nseparatedGamma %&gt;%\n  mutate(title = reorder(Event, gamma * topic)) %&gt;%\n  ggplot(aes(factor(topic), gamma)) +\n  geom_boxplot() +\n  facet_wrap(~ Event) +\n  labs(x = \"topic\", y = expression(gamma))\n\n\n\n\n\n\n\n\nLooking at the graph we can clearly see that there are 3 Events with significantly lower mis-attribution of articles/headlines they are: The Farmington New Mexico shooting, the judgment against Jennifer Crumbley and the Prague university shooting."
  },
  {
    "objectID": "SlerkaProjekt.html#conclusion",
    "href": "SlerkaProjekt.html#conclusion",
    "title": "Analyzing the CNN coverage of the Prague University shooting",
    "section": "Conclusion",
    "text": "Conclusion\nThe uniqueness of these three events is understandable; the shooting in Farmington was unique in the age of its victims (73-97) and the method of its perpetrator, the case against Jenifer Crumbley was poised to set a precedent in the USA (by now it has set it already) and the shooting in Prague has shown the danger of exporting the motivations and conditions of mass shooting from the USA into other countries with similar cultural contexts."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Specializing in the study of work precarization and its impact on young adults in the gig and digital economies, demonstrating a deep understanding of contemporary labor issues and trends.\nParticipating in seminars and workshops on digital transformation and its impact on labor markets, in an attempt at understanding recent and predicting future societal changes.\nEngaging in interdisciplinary coursework that combines sociology, economics, labor studies, and politology.\n\n\n\n\n\n\n\n\nEngaged in multidisciplinary projects involving urban planning and sustainable design, honing collaboration, and project management skills, emphasizing the ability to work effectively in team settings.\nUtilized advanced software and technology for architectural design and urban planning, such as ARCHICAD21, Revit, and GIS.\n\n\n\n\n\n\n\n\nFinished with an Average of 1.4. (German grading system)"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Specializing in the study of work precarization and its impact on young adults in the gig and digital economies, demonstrating a deep understanding of contemporary labor issues and trends.\nParticipating in seminars and workshops on digital transformation and its impact on labor markets, in an attempt at understanding recent and predicting future societal changes.\nEngaging in interdisciplinary coursework that combines sociology, economics, labor studies, and politology.\n\n\n\n\n\n\n\n\nEngaged in multidisciplinary projects involving urban planning and sustainable design, honing collaboration, and project management skills, emphasizing the ability to work effectively in team settings.\nUtilized advanced software and technology for architectural design and urban planning, such as ARCHICAD21, Revit, and GIS.\n\n\n\n\n\n\n\n\nFinished with an Average of 1.4. (German grading system)"
  },
  {
    "objectID": "cv.html#work-experience",
    "href": "cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE\n\nPAQ Research\n\nData analyst | Jan 2022 – Present\nData analysis and topical research. Mainly data analysis and cleaning using R, with occasional Python for interactive web applications. Questionnaire design, coding and testing. Topical research of both academic and governmental sources. Key results:\n\nResearch for “Hlas Ukrajinců” (Voice of Ukrainians) an project mapping the integration of the incoming refugees from Ukraine in the Czech Republic.\nResearch and Questionnaire design for “Praha 1 očima občanů” (Prague 1 through the eyes of its citizens) a comparative study on the effects of over-tourism in Prague 1 on the quality of life for it’s inhabitants.\nData finding, cleaning and wrangling to put into the internal database of PAQ, and publishing a select subset on DataPAQ.\n\n\n\n\nInbaze\n\nExternal German Lecturer | Oct 2020 – Present\nParticipation in the Project “Bedýnky příběhů” Storyboxes a program aimed to increase the children’s familiarity and understanding of foregin languages and cultures. The program is aimed at children from 6 to 10 years old, and is based on the idea of a “story-box” - a box filled with items and stories that are related to a country, and are used to engage the children with the given culture.\n\n\n\nFreelance Translator\n\nTranslator | 2016 – Present\n\n\n\nEmbassy of the Czech Republic in Berlin\n\nTranslator | 2018\n\nTranslated various theater pieces from Czech to German. - Cooperated with Mrs. Barbora Schnelle in their dramatization as a part of the Czech Culture festival “Ein Stück: Tschechien” One Piece: Czechia."
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "SKILLS",
    "text": "SKILLS\nLanguages\nGerman: Native, Czech: Native, English: C2, French: B1\nProgramming\nR: Experienced, Python: Intermediate\nDesign Software\nARCHICAD21, Revit, GIS, Blender3D, Rhino\nOther Software\nPhotoshop, InDesign, Inkscape, LaTex, Microsoft Office"
  },
  {
    "objectID": "cv.html#references",
    "href": "cv.html#references",
    "title": "Curriculum Vitae",
    "section": "REFERENCES",
    "text": "REFERENCES\nAvailable upon request"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello, I am Matyáš Levínský a sociology student at the philosophical faculty of the Charles University in Prague. I am primarily interested in the precarization of work due to the Gig economy, and AI. I try to follow hot topics, and as such am interested in AI itself.\nSince 2022 I am working at PAQ Research as a Data Analyst. You may find my entire CV here.\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Ongoing restructuring\n\n\n\nThis site is being just created, things may move around or not be functional. Thanks for your patience.\nHello, I am Matyáš Levínský a sociology student at the philosophical faculty of the Charles University. This page serves to show rendered content from my repositories and is currently a work in progress."
  },
  {
    "objectID": "index.html#what-did-i-upload",
    "href": "index.html#what-did-i-upload",
    "title": "Welcome",
    "section": "What did I upload?",
    "text": "What did I upload?\n\nFebruary 2024\nCNN reporting analysis\nI investigated how CNN reported on the mass shooting that happened on the 21.12.2023 at my University."
  }
]